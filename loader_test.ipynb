{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "import torch.autograd as autograd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from csv_handle import save_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_file=\"data/facebook.csv\"\n",
    "version = \"_fb\"\n",
    "train_filename, valid_filename, test_filename = save_csv(source_file=source_file, version=version, k=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tv_datafields = [(\"comment_text\", TEXT),\n",
    "                 (\"gender\", LABEL)]\n",
    "\n",
    "trn, vld = TabularDataset.splits(\n",
    "        path=\"\", # the root directory where the data lies\n",
    "        train=train_filename, validation=valid_filename,\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tv_datafields)\n",
    "\n",
    "tst_datafields = [(\"comment_text\", TEXT),\n",
    "                 (\"gender\", LABEL)]\n",
    "\n",
    "tst = TabularDataset(\n",
    "        path=test_filename, # the file path\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tst_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 25517),\n",
       " ('to', 20234),\n",
       " ('and', 15195),\n",
       " ('you', 13843),\n",
       " ('a', 12721),\n",
       " ('of', 11757),\n",
       " ('in', 11419),\n",
       " ('for', 9584),\n",
       " ('i', 8747),\n",
       " ('is', 8257)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = TEXT.vocab\n",
    "vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (trn, vld), # we pass in the datasets we want the iterator to draw data from\n",
    "        batch_sizes=(64, 64),\n",
    "        device=-1, # if you want to use the GPU, specify the GPU number here\n",
    "        sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")\n",
    "\n",
    "test_iter = Iterator(tst, batch_size=64, device=-1, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_num, embed_dim=100, class_num=2, kernel_num=200, kernel_sizes=[1,2,3,4]):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        \n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        C = class_num\n",
    "        Ci = 1\n",
    "        Co = kernel_num\n",
    "        Ks = kernel_sizes\n",
    "\n",
    "        \n",
    "        self.embed = nn.Embedding(V, D, scale_grad_by_freq=True, padding_idx=1)\n",
    "        \n",
    "        #self.embed.weight.data.copy_(args.pretrained_weight)\n",
    "        self.embed.weight.requires_grad = True\n",
    "        print(\"dddd {} \".format(self.embed.weight.data.size()))\n",
    "\n",
    "        print(\"using wide convolution\")\n",
    "        self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), stride=(1, 1),\n",
    "                                     padding=(K//2, 0), dilation=1, bias=False) for K in Ks]\n",
    "        print(self.convs1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.dropout_embed = nn.Dropout(p=0.5)\n",
    "        in_fea = len(Ks) * Co\n",
    "        self.fc = nn.Linear(in_features=in_fea, out_features=C, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (N,W,D)\n",
    "        x = self.dropout_embed(x)\n",
    "        x = x.unsqueeze(1)  # (N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  # (N,len(Ks)*Co)\n",
    "        \n",
    "        logit = self.fc(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_num, lstm_hidden_dim=200, lstm_num_layers=1, embed_dim=100, class_num=2):\n",
    "        super(BiGRU, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        self.num_layers = lstm_num_layers\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        C = class_num\n",
    "\n",
    "\n",
    "        self.embed = nn.Embedding(V, D, padding_idx=1)\n",
    "        # pretrained  embedding\n",
    "        #self.embed.weight.data.copy_(args.pretrained_weight)\n",
    "        # gru\n",
    "        self.bigru = nn.GRU(D, self.hidden_dim, dropout=0.6, num_layers=self.num_layers, bidirectional=True)\n",
    "        # linear\n",
    "        self.hidden2label = nn.Linear(self.hidden_dim * 2, C)\n",
    "        #  dropout\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embed = self.embed(input)\n",
    "        embed = self.dropout(embed)\n",
    "        input = embed.view(len(input), embed.size(1), -1)\n",
    "        # gru\n",
    "        gru_out, _ = self.bigru(input)\n",
    "        gru_out = torch.transpose(gru_out, 0, 1)\n",
    "        gru_out = torch.transpose(gru_out, 1, 2)\n",
    "        # pooling\n",
    "        # gru_out = F.tanh(gru_out)\n",
    "        gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)\n",
    "        gru_out = F.tanh(gru_out)\n",
    "        # linear\n",
    "        y = self.hidden2label(gru_out)\n",
    "        logit = y\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=100,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        #self.embedding.weight.data.copy_(vocab.vectors)\n",
    "        \n",
    "        #print (self.embedding)\n",
    "        \n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 2)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CNN_Text(len(vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-8)\n",
    "epochs = 10\n",
    "is_CNN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = BiGRU(len(vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-8)\n",
    "epochs = 10\n",
    "is_CNN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTM(len(vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-8)\n",
    "epochs = 10\n",
    "is_CNN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## The 1 Epoch, All 10 Epochs ! ##\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/469 [00:19<18:51,  2.45s/it]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c0f148b239e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    steps = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    print(\"\\n## The {} Epoch, All {} Epochs ! ##\".format(epoch, epochs))\n",
    "    model.train()\n",
    "    for batch in tqdm.tqdm(train_iter):\n",
    "        feature, target = batch.comment_text, batch.gender\n",
    "        \n",
    "        if is_CNN:\n",
    "            feature.data.t_()\n",
    "        else:\n",
    "            target = autograd.Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # model.zero_grad()\n",
    "\n",
    "        logit = model(feature)\n",
    "        F.cross_entropy(logit, target)\n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data.item() * feature.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for batch in tqdm.tqdm(val_iter):\n",
    "        x, y = batch.comment_text, batch.gender\n",
    "        preds = model(x)\n",
    "        loss = F.cross_entropy(preds, y)\n",
    "        val_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:04<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for batch in tqdm.tqdm(test_iter):\n",
    "    x, y = batch.comment_text, batch.gender\n",
    "    x.data.t_()\n",
    "    preds = model(x)\n",
    "    preds = preds.data.numpy()\n",
    "    \n",
    "    pair = [(np.argmax(item), v.item()) for item, v in zip(preds, y)]\n",
    "    pairs.extend(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5070140280561122"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([(1 if (x==y) else 0) for x,y in [((1 if (x>0.5) else 0), y) for x, y in pairs]])/len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.21it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for batch in tqdm.tqdm(train_iter):\n",
    "    x, y = batch.comment_text, batch.gender\n",
    "    x.data.t_()\n",
    "    preds = model(x)\n",
    "    preds = preds.data.numpy()\n",
    "    \n",
    "    pair = [(np.argmax(item), v.item()) for item, v in zip(preds, y)]\n",
    "    pairs.extend(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5148580968280467"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([(1 if (x==y) else 0) for x,y in [((1 if (x>0.5) else 0), y) for x, y in pairs]])/len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
